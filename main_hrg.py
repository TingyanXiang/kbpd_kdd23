# coding=utf-8
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import sys
from collections import defaultdict
from functools import partial
from operator import itemgetter
from tqdm import tqdm
import graphlearn as gl
import graphlearn.python.nn.pytorch as thg
from torch_geometric.data import HeteroData
from utils.gl_io_utils import HeteGraph
from model.RGCN import *
from utils.utils import bool_flag, printWithTimes, save_model, load_model

argv = sys.argv[0:]
# import and process config
from config.config_hrg import config, graph_config
cur_path = os.getcwd() #directory when running; by defualt, =directory where cloning this repository.
print('cur_path: ', cur_path)
if graph_config.get('data_root_path', None) is None:
    graph_config['data_root_path'] = os.path.join(cur_path, 'kbpd_kdd23_open_code_v2', 'data')
if config.get('save_root_path', None) is None:
    config['save_root_path'] = os.path.join(cur_path, 'saved_models')
if len(argv)>1:
    config['mode'] = argv[1]
print('config: ', config)

# running environment setting
if torch.cuda.is_available(): config['device'] = 'cuda'
else: config['device'] = 'cpu'
device = torch.device(config['device'])
world_size = 1
rank = 0
# torch distribution setting
if bool_flag(config.get('ddp',False)):
    if torch.cuda.is_available():
        torch.distributed.init_process_group('nccl')
    else:
        torch.distributed.init_process_group('gloo')
    world_size = torch.distributed.get_world_size()
    rank = torch.distributed.get_rank()

# define model
print('build model...')
if config['model_choice'] == 'RGCN':
    model = RGCN(
        in_channels = config['node_feat_num'],
        hidden_channels = config['hidden_channels'],
        num_class = config['num_class'],
        num_relations = config['num_relations'],
        metadata=None,
        n_bases=config['n_bases'],
        n_layers=config['gnn_num_layers'],
        out_layer_num=config['out_layer_num'],
        dropout_rate=config['dropout_rate'],
        default_label=graph_config['default_label']
    )
else:
    raise IOError("config['model_choice']=={model_choice} is out of range!!!".format(model_choice=config['model_choice']))

print('build model done.')
if world_size > 1:
    model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)
print(model)

# choose optimizer and loss function
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.0)

# init graph through gl
print("init graph...")
gl.set_tape_capacity(1)
hete_graph = HeteGraph(graph_config)
gl.set_tracker_mode(0)
hete_graph.init_gl(rank, world_size)
print("init graph done.")

def induce_func(edge_types_samplingconfig, sample_node_type, data_dict):
    '''
    transform batch data in dict to pyg data list.
    params
    >> data_dict: batch data in dict generated by graph-learn query
        data_dict['src']
        data_dict['src_hop1_'+edge_type]
        has attrsï¼š
            ids
            int_attrs
            float_attrs
            string_attrs
            weights
            labels
    return
    >> subgraph_list: list of subgraphs in pyg data format
    '''
    src = data_dict['src']
    default_neighbor_id = graph_config['default_neighbor_id']
    default_label = graph_config['default_label'] #default_for nodes without labels
    node_dict = defaultdict(list)
    edge_dict = defaultdict(list)

    node_dict[sample_node_type].append(src.ids)
    for etype, config in edge_types_samplingconfig.items():
        nbrs_num = config['nbrs_num']
        lf = data_dict['src_hop1_'+etype]
        nbr_ids = lf.ids #(bz, nbrs_num)

        # get edge link
        t_index = src.ids.repeat(nbrs_num)
        s_index = nbr_ids.flatten()
        edge_link = np.stack((s_index, t_index), axis=1)
        edge_dict[etype].append(edge_link)
        # undirected, add inverse edge
        edge_dict[etype].append(np.flip(edge_link, axis=1))
        # nbr_ids = np.unique(nbr_ids)
        nbr_ids = np.setdiff1d(nbr_ids, [default_neighbor_id])
        if len(nbr_ids) > 0:
            node_dict[sample_node_type].append(nbr_ids)

    data = HeteroData()
    # concat and unique the node dict
    # build node map and get node feature
    node_maps = defaultdict(dict)
    for ntype in [sample_node_type]:
        # concat and unique
        if len(node_dict[ntype]) <= 0:
            data[ntype].x = torch.zeros((0, config['node_feat_num']))
            label = torch.ones(0) * default_label
            data[ntype].y = label.long()
            continue
        temp = np.unique(np.concatenate(node_dict[ntype], axis=-1))
        node_dict[ntype] = temp
        data[ntype].org_ids = torch.LongTensor(temp)
        # get node feature
        n_features = hete_graph.g.get_nodes(ntype, temp)
        n_features = n_features.float_attrs
        data[ntype].x = torch.FloatTensor(n_features)
        label = torch.ones(len(data[ntype].x)) * default_label
        data[ntype].y = label.long()
        # bulid node map
        node_maps[ntype] = {k: v for v, k in enumerate(temp)}

    # concat and unique the edge dict
    # reindex edge
    # edge_indexs = {}
    for stype, etype, ttype in graph_config['meta_data'][1]:
        edge_tuple = (stype, etype, ttype)
        if len(edge_dict[etype]) <= 0:
            data[edge_tuple].edge_index = torch.LongTensor([[], []])
            continue
        # concat
        temp = np.concatenate(edge_dict[etype], axis=0)
        # unique
        temp = temp[temp[:, 0] != default_neighbor_id]
        temp = temp[temp[:, 1] != default_neighbor_id]
        unique_temp = temp[:, 0] + temp[:, 1] * 1j
        unique_index = np.unique(unique_temp, return_index=True)[1]
        temp = temp[unique_index]
        if len(temp) <= 0:
            data[edge_tuple].edge_index = torch.LongTensor([[], []])
            continue
        # reindex
        s_reindex = itemgetter(*temp[:, 0])(node_maps[stype])
        t_reindex = itemgetter(*temp[:, 1])(node_maps[ttype])
        s_reindex = np.array(s_reindex)
        t_reindex = np.array(t_reindex)
        reindex = np.stack((s_reindex, t_reindex), axis=1)
        reindex = torch.LongTensor(reindex)
        data[edge_tuple].edge_index = reindex.t().contiguous()

    # get root reindex
    root_index = itemgetter(*src.ids)(node_maps[sample_node_type])
    root_index = np.array(root_index)
    root_index = torch.LongTensor(root_index)
    root_label = torch.LongTensor(src.labels)
    # get root lable
    data[sample_node_type].y[root_index] = root_label
    # data['root_index'] = root_index
    # # get oral root ids to align the output
    # _, indices = torch.sort(root_index)
    # batch_ids = torch.LongTensor(src.ids)
    # batch_ids = batch_ids[indices]
    # data['oral_batch_ids'] = batch_ids
    return [data, ]

mask = graph_config['tables_dict']['sample']['hrg_sample_table']['mask']
print('mask: ', mask)
edge_types_samplingconfig = {}
for stype,rtype,ttype in graph_config['meta_data'][1]:
    edge_types_samplingconfig[rtype] = {'nbrs_num': config['nbrs_num']}
query = hete_graph.feed_next_w1hop(config['batch_size'], node_type='samplenode', mask=mask, edge_types_config=edge_types_samplingconfig)
dataset = thg.Dataset(query, window=5, induce_func=partial(induce_func, edge_types_samplingconfig, 'samplenode'))
count_per_server = hete_graph.g.server_get_stats()[gl.get_mask_type('samplenode', mask)]
print('sample count per server: ', count_per_server)
num_step_per_epoch = min(count_per_server) // config['batch_size']
print('num_step_per_epoch being set to: ' + str(num_step_per_epoch))

if config['mode'] == 'train':
    print("train ...")
    train_loader = thg.PyGDataLoader(dataset, length=num_step_per_epoch)
    model.train()

    loss_list = []
    for epoch in range(config['num_epoch']):
        step = 0
        for i, data in tqdm(enumerate(train_loader)):
            step += 1
            data = data.to(device)
            logits, y, _ = model(data)
            loss = criterion(logits, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            loss_list = loss_list[-10:]+[loss.item(),]
            if (epoch==0 and step<=50) or step % config['num_step_print'] == 0:
                print('batch_labels:', y.shape, np.sum(y.cpu().detach().numpy()))
                printWithTimes('Epoch:[%d/%d]:, Step:[%d/%d] Loss: %.4f avgLoss: %.4f' % (epoch, config['num_epoch'], step, num_step_per_epoch, loss.item(), np.mean(loss_list)))
    # save model
    save_model(model, config['save_root_path'], config['model_name'])

if config['mode'] == 'test':
    test_loader = thg.PyGDataLoader(dataset)
    writer = open(os.path.join(config['save_root_path'], config['model_name'],'output.txt'),'w')

    #load model
    model = load_model(model, config['save_root_path'], config['model_name'])
    model.eval()

    for epoch in range(1):
        step = 0
        for i, data in tqdm(enumerate(test_loader)):
            step += 1

            data = data.to(device)
            logits, y, org_batch_ids = model(data)

            values = []
            ids = [str(id) for id in org_batch_ids.cpu().detach().numpy()]
            labels = [str(label) for label in y.cpu().detach().numpy()]

            predictions = []
            for p in logits.cpu().detach().numpy():
                predictions.append(','.join([str(e) for e in p]))

            assert (len(ids) == len(labels) == len(predictions))
            values = ["\t".join([str(id), str(label), str(p)]) for id, label, p in zip(ids, labels, predictions)]
            writer.write("\n".join(values)+"\n")

            if (step) % config['num_step_print'] == 0:
                printWithTimes('Step [%d/%d]' % (step, num_step_per_epoch))

    writer.close()

hete_graph.g.close()



